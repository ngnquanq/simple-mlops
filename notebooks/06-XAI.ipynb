{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Kinda useless ngl ._.\n",
    "# logging.basicConfig(filename='preprocessing.log',\n",
    "#                     filemode='a',\n",
    "#                     format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "#                     datefmt='%H:%M:%S',\n",
    "#                     level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Abstract class is a strategy for handling data\n",
    "class DataStrategy(ABC):\n",
    "    \"\"\"Abstract class defining strategy for handling data\n",
    "\n",
    "    Args:\n",
    "        ABC (_type_): _description_\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def handle_data(self, data: pd.DataFrame) -> Union[pd.DataFrame, pd.Series]:\n",
    "        pass \n",
    "    \n",
    "    \n",
    "class DataPreProcessStrategy(DataStrategy):\n",
    "    \"\"\"Inherit the datastrategy and overwrite the handle_data method provided by the DataStrategy above\"\"\"\n",
    "    \n",
    "    def handle_data(self, data: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\"Preprocess the dataframe\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame that need to be preprocessed.\n",
    "        \"\"\"\n",
    "        # Drop useless coloumns\n",
    "        logger.info(\"Begin to preprocessing the dataframe ...\")\n",
    "        try: \n",
    "            logger.info(\"1. Start dropping useless columns\")\n",
    "            data = data.drop(columns=[\n",
    "                \"Account length\", \n",
    "                \"State\", \n",
    "                \"Area code\"\n",
    "            ])\n",
    "            logger.info(\"Delete useless columns complete\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Encounting an exception when dropping columns\")\n",
    "            raise e\n",
    "        \n",
    "        # Convert data type.\n",
    "        try: \n",
    "            # Converting object column to category\n",
    "            logging.info(\"2. Converting data to it correct data types\")\n",
    "            for i in data.select_dtypes(include='object').columns.to_list(): \n",
    "                data[i] = data[i].astype('category')\n",
    "            # Converting target column to category \n",
    "            data['Churn'] = data['Churn'].astype('category')\n",
    "            logging.info(\"Converting data type complete\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Encounting an exception when convert data type\")\n",
    "            raise e\n",
    "        \n",
    "        # Handling null value.\n",
    "        try: \n",
    "            if data.isnull().sum().any()==True:\n",
    "                logging.info (\"3. Handling null values\")   \n",
    "                for i in data.select_dtypes(include=['int64', 'float64']).columns.to_list():\n",
    "                    data[i].fillna(data[i].mean(), inplace=True)\n",
    "                data = data.dropna()\n",
    "            else: \n",
    "                logging.info(\"3. The data had no missing values\")\n",
    "        except Exception as e: \n",
    "            logger.exception(f\"Encounting an exception when handling null value\")\n",
    "        \n",
    "        # Scale if needed. \n",
    "        # Identify numerical columns\n",
    "        logging.info('4. Encoding the values')\n",
    "        num_col = data.select_dtypes(include=['int64', 'float64']).columns.values.tolist()\n",
    "\n",
    "        # Identify categorical columns \n",
    "        cat_col = data.select_dtypes(include='category').columns.values.tolist()\n",
    "\n",
    "        # Encoding the data\n",
    "        numeric_transformer = Pipeline(\n",
    "            steps=[(\"Scaler\", StandardScaler())]\n",
    "        )\n",
    "\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[('OneHotEncoder', OneHotEncoder(handle_unknown='ignore'))]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, num_col),\n",
    "                ('cat', categorical_transformer, cat_col)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        encoded_data = preprocessor.fit_transform(data)\n",
    "        \n",
    "        # Create new names\n",
    "        # Keep OG names for num_col\n",
    "        new_num_col = num_col\n",
    "        \n",
    "        # Change name for cat_col \n",
    "        new_cat_col = preprocessor.named_transformers_['cat'].named_steps['OneHotEncoder'].get_feature_names_out(cat_col)\n",
    "        \n",
    "        # Combine to have new col names\n",
    "        columns = list(new_num_col) + list(new_cat_col)\n",
    "        \n",
    "        encoded_data = pd.DataFrame(encoded_data, columns=columns)\n",
    "        return encoded_data\n",
    "    \n",
    "\n",
    "class DataDivideStrategy(DataStrategy): \n",
    "    \"\"\"Split the data into the dataframes for training and testing process. \n",
    "\n",
    "    Args:\n",
    "        Dataframe (pd.DataFrame): Take in the already encoded dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    def handle_data(self, data) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        data = pd.DataFrame(data)\n",
    "        X = data.iloc[:, :-2]  # All columns except the last two\n",
    "        y = data.iloc[:, -2:]  # The last two columns\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "        return X_train, y_train, X_valid, y_valid\n",
    "    \n",
    "class DataCleaning(DataStrategy):\n",
    "    \"\"\"\n",
    "    Data cleaning class which preprocesses the data and divides it into train and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, strategy: DataStrategy) -> None:\n",
    "        \"\"\"Initializes the DataCleaning class with a specific strategy.\"\"\"\n",
    "        self.df = data\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def handle_data(self):\n",
    "        \"\"\"Handle data based on the provided strategy\"\"\"\n",
    "        return self.strategy.handle_data(self.df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import logging \n",
    "from zenml import step \n",
    "from typing import Tuple, Annotated\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> Tuple[\n",
    "    Annotated[pd.DataFrame, 'X_train'],\n",
    "    Annotated[pd.DataFrame, 'X_valid'],\n",
    "    Annotated[pd.DataFrame, 'y_train'],\n",
    "    Annotated[pd.DataFrame, 'y_valid']\n",
    "]:\n",
    "    try: \n",
    "        process_strategy = DataPreProcessStrategy()\n",
    "        data_cleaning = DataCleaning(df, process_strategy)\n",
    "        cleaned_data = data_cleaning.handle_data()\n",
    "        \n",
    "        divide_strategy = DataDivideStrategy()\n",
    "        data_dividing = DataCleaning(cleaned_data, divide_strategy)\n",
    "        dataframe = data_dividing.handle_data()\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning data: {e}\")\n",
    "        raise e \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mBegin to preprocessing the dataframe ...\u001b[0m\n",
      "\u001b[1;35m1. Start dropping useless columns\u001b[0m\n",
      "\u001b[1;35mDelete useless columns complete\u001b[0m\n",
      "\u001b[1;35m2. Converting data to it correct data types\u001b[0m\n",
      "\u001b[1;35mConverting data type complete\u001b[0m\n",
      "\u001b[1;35m3. The data had no missing values\u001b[0m\n",
      "\u001b[1;35m4. Encoding the values\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Read and transform the data\n",
    "dataframe = pd.read_csv('../data/telecom_churn.csv')\n",
    "X_train, y_train, X_test, y_test = clean_data(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "from typing import Dict \n",
    "import pandas as pd \n",
    "import typing\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='logs/Evaluate.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Evaluation(ABC): \n",
    "    @abstractmethod\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        pass \n",
    "    \n",
    "\n",
    "class ConfusionMatrix(Evaluation):\n",
    "    \"\"\"Evaluattion Strategy that used Confusion Matrix\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str , np.ndarray]:\n",
    "        try: \n",
    "            # Convert DataFrame or Series to NumPy array\n",
    "            if isinstance(y_true, (pd.DataFrame, pd.Series)):\n",
    "                y_true = y_true.to_numpy().ravel()\n",
    "            if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n",
    "                y_pred = y_pred.to_numpy().ravel()\n",
    "            logger.info('Calculating Confusion matrix')\n",
    "            cm = confusion_matrix(y_true=y_true.argmax(axis=1), y_pred=y_pred.argmax(axis=1))\n",
    "            logger.info('Calculate confusion matrix complete')\n",
    "            return {'Confusion matrix': cm}\n",
    "        except Exception as e: \n",
    "            logging.error(\"Error in calculating Confusion matrix: {}\".format(e))\n",
    "            raise e \n",
    "        \n",
    "\n",
    "class ClassificationMetrics(Evaluation):\n",
    "    \"\"\" Evaluate strategy that uses other metrics like AUC, accuracy, etc. \"\"\"\n",
    "    \n",
    "    def calculate_score(self, y_true: typing.Union[np.ndarray, pd.Series, pd.DataFrame], \n",
    "                        y_pred: typing.Union[np.ndarray, pd.Series, pd.DataFrame]) -> Dict[str, float]:\n",
    "        try:\n",
    "            # Convert DataFrame or Series to NumPy array\n",
    "            if isinstance(y_true, (pd.DataFrame, pd.Series)):\n",
    "                y_true = y_true.to_numpy().ravel()\n",
    "            if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n",
    "                y_pred = y_pred.to_numpy().ravel()\n",
    "            \n",
    "            logger.info('Calculating other metrics')\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, average='micro')\n",
    "            recall = recall_score(y_true, y_pred, average='micro')\n",
    "            precision = precision_score(y_true, y_pred, average='micro')\n",
    "            logger.info('Calculate metrics complete')\n",
    "            return {\n",
    "                'accuracy': accuracy, \n",
    "                'f1': f1, \n",
    "                'recall': recall, \n",
    "                'precision': precision\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in calculating other metrics: {}\".format(e))\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import step\n",
    "import pandas as pd\n",
    "import logging \n",
    "from typing import Union, Tuple, Annotated\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "logging.basicConfig(filename='logs/evaluate.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: ClassifierMixin,\n",
    "    feature: Union[pd.DataFrame | pd.Series | np.ndarray],\n",
    "    target: Union[pd.DataFrame | pd.Series] | np.ndarray\n",
    "    ) -> Tuple[\n",
    "        Annotated[dict, \"Confusion matrix\"] ,\n",
    "        Annotated[dict, \"Other metrics\"]\n",
    "        ]: \n",
    "    prediction = model.predict(feature)\n",
    "    cm_class = ConfusionMatrix()\n",
    "    cm = cm_class.calculate_score(y_true=target, y_pred=prediction)\n",
    "\n",
    "    metric_class = ClassificationMetrics()\n",
    "    metrics = metric_class.calculate_score(y_true=target, y_pred=prediction)\n",
    "    \n",
    "    return cm, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI with these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
